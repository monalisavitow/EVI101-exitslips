---
title: 'EVI101 Echus Exit Slips: Session 1 Welcome'
author: "Jackie Ramos-Draper"
date: "2023-08-14"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Prep Steps:
#1. Download data from Qualtrics (I filtered to just our section + session), and save this somewhere. It'll save as a zip, so open it to unzip before reading it into here.

library(tidyverse)
setwd("~/Desktop/Harvard/EVI101/Exit Slips/Day 1")
echus_1 <- read_csv("Evidence exit ticket Aug 2023_August 14, 2023_17.52.csv")

#remove junk 2 tops rows
echus_1 <- echus_1[-1,]
echus_1 <- echus_1[-1,]

#reduce to just our section
echus_1 <- echus_1 %>% filter( Q1 == "Echus (---)")

#remove columns
echus_1 <- echus_1 %>% select(-c(StartDate:RecordedDate, RecipientLastName:UserLanguage))
```

\ 
\ 
\ 
\ 

Question 1 and 2 are for section + session, so results start at Q3.

\ 
\ 
\ 

## Q3: Overall, how did class go today?  (n = 63)

```{r Q3, echo=FALSE}
echus_1$Q3 <- as.factor(echus_1$Q3)
#table(echus_1$Q3)
#sum(!is.na(echus_1$Q3)) #get n
echus_1$Q3 <- factor(echus_1$Q3,                                    
                  # Change ordering manually
                  levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
#table(echus_1$Q3)

ggplot(data=echus_1, aes(x=Q3)) +
  geom_bar(fill = "#66CC99") +
  theme_minimal() + 
  labs(title = "How did class go today?", 
       x = "") + 
  geom_text(stat='count', aes(label=..count..), vjust=-0.25)
```

\newpage

## Q4: What went well? n=55
```{r Q4, echo=FALSE, eval=FALSE}
sum(!is.na(echus_1$Q4)) #right 
head(echus_1$Q4, 55)
```

**1. Groups + Group Discussions**  

[insert sample responses]


**2. Structure and Pace of Class**

[insert sample responses]


**3. Instructors**

[insert sample responses]


\newpage

## Q5: What could we improve? (n=43)
```{r Q5, echo=FALSE, eval=FALSE}
sum(!is.na(echus_1$Q5)) #43 out of 62 people responded 
head(echus_1$Q5, 43)
```

**1. Timing for Team Discussions**

[insert sample responses]
                                                                         
                                    
**2. Instruction Insights**

[insert sample responses]

                                                                                                                              

**3. Pace in Lecture**

[insert sample responses]
         


**4. Operations** 
[insert sample responses]

```{r echo=FALSE, eval=FALSE}
echus_temp <- echus_1 %>% filter(Q3 == "Fair")
echus_temp$Q5
```
*Student who rated class as "Fair" for Q3 said this for Q5: [insert sample responses]*


\newpage

## Q6: Is your team working well together? (n = 63)  

```{r Q6, echo=F}
#echus_1$Q6 <- as.factor(echus_1$Q6)
#sum(!is.na(echus_1$Q6)) #get n


ggplot(data=echus_1, aes(x=Q6, fill = Q6)) +
  geom_bar() +
  theme_minimal() + 
  labs(title = "Is your team working well together?", 
       x = "") + 
  geom_text(stat='count', aes(label=..count..), vjust=-0.25)

```

## Q7: If you'd like a member of the teaching team to reach out to help with your team's dynamics, please add your full name here. 
```{r, echo=FALSE}
echus7 <- echus_1 %>% filter(Q7 !="")
head(echus7$Q7)
```

\newpage

# Appendix 
```{r Q4: nlp, echo=FALSE, eval=FALSE}
#NLP
require(quanteda)
require(ggplot2)
require(stm)
require(tidyverse)
require(geomtextpath)
require(quanteda.textstats)
require(udpipe)

echus4 <- echus_1 %>% filter(Q4 !="")

corpus_evi <- corpus(x = echus4$Q4, 
                     docnames = echus4$ResponseId)


dfm <- corpus_evi %>%
  tokens(remove_punct = TRUE,  #remove punctuation
         remove_numbers = TRUE, #remove any numbers --- maybe not the best move if they put a cost 
         remove_symbols = TRUE) %>% # remove uninformative tokens
  tokens_tolower(keep_acronyms = T) %>% # casefold tokens, keep acronyms
  tokens_remove(c(stopwords("en"), "really", "like", "love", "lots", "can", "appreciate", "well", "loved", "great", "enjoyed", "got", "get", "good", "getting", "feel")) %>% #remove common stopwords in English
  #tokens_wordstem() %>% # ADDED THIS
  dfm()


toks.df <- textstat_frequency(dfm)
toks.df

#setwd("~/Desktop")
#ud_model <- udpipe_load_model("english-ewt-ud-2.3-181115.udpipe")
# phrase.toks <- udpipe_annotate(object = ud_model, 
#                         x = echus4$Q4)

setwd("~/Desktop/Harvard/EVI101/Exit Slips/Day 1")
#phrase.toks <- as.data.frame(phrase.toks)
#head(phrase.toks[,c("token", "upos")],20)
#relevant.pos <- c("NOUN","PROPN","VERB","ADJ","SYM") 
#filter.condition <- phrase.toks$upos %in% relevant.pos

# key.phrases.0 <- keywords_rake(x = phrase.toks,
#                              term = "token", 
#                              group = "doc_id",
#                              relevant = filter.condition,
#                              ngram_max = 2, 
#                              n_min = 2) 

#head(key.phrases.0, 50) #not super useful list, so i'll only reprocess for wordstem 

dfm <- corpus_evi %>%
  tokens(remove_punct = TRUE,  #remove punctuation
         remove_numbers = TRUE, #remove any numbers --- maybe not the best move if they put a cost 
         remove_symbols = TRUE) %>% # remove uninformative tokens
  tokens_tolower(keep_acronyms = T) %>% # casefold tokens, keep acronyms
  tokens_remove(c(stopwords("en"), "really", "like", "love", "lots", "can", "appreciate", "well", "loved", "great", "enjoyed", "got", "get", "good", "getting", "feel", "know")) %>% #remove common stopwords in English that you're seeing 
  tokens_wordstem() %>% # ADDED THIS
  dfm()



#convert dfm to stm object
tm.dfm <- convert(dfm, to = "stm" )

class(tm.dfm)
names(tm.dfm)

#k results
kresult <- searchK(tm.dfm$documents, tm.dfm$vocab, K = c(3:16)) 
plot(kresult) #looking for lower HOL, higher SC --  6? 10?

mod <- stm(documents = tm.dfm$documents, 
                   vocab = tm.dfm$vocab,
                   K = 4) 
#^ confirms qual number of topics

plot(x = mod, 
     topics = 1:4,
     type = "summary",
     labeltype = "prob",
     main = "prob")

plot(x = mod, 
     topics = 1:4,
     type = "summary",
     labeltype = "frex",
     main = "frex")


#Label Topics
labelTopics(model = mod, topics = 1, n = 10)
labelTopics(model = mod, topics = 2, n = 10) 
labelTopics(model = mod, topics = 3, n = 10)
labelTopics(model = mod, topics = 4, n = 10) 

findThoughts(model = mod,
             texts = echus4$Q4,
             topics = 1,
             n = 10)

findThoughts(model = mod,
             texts = echus4$Q4,
             topics = 2,
             n = 10)

findThoughts(model = mod,
             texts = echus4$Q4,
             topics = 3,
             n = 10)

findThoughts(model = mod,
             texts = echus4$Q4,
             topics = 4,
             n = 10)

#plotting topic highest words
library(tidytext)
word_topics <- tidy(mod, matrix = "beta")
class(word_topics)
names(word_topics)

word_topics %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 10) %>% 
  ungroup() %>% 
  mutate(topic = paste("Topic", topic)) %>% 
  ggplot(aes(beta, reorder_within(term, beta, topic), fill = topic)) +
  geom_col(show.legend = F) +
  facet_wrap(vars(topic), scales = "free_y") +
  scale_x_continuous( expand = c(0,0)) + 
  scale_y_reordered() +
  labs(x = expression(beta), y = NULL,
       title = "Highest probability words by topic") 

#go in manually to say group = team

```

```{r echo=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

**Q4: What went well?**  
```{r Q4 all responses, echo=FALSE}
#sum(!is.na(echus_1$Q4))
echus4 <- echus_1 %>% filter(Q4 !="")
head(echus4$Q4, 55)
```

\newpage

**Q5: What could we improve?**
```{r Q5 all responses, tidy=TRUE, tidy.opts=list(width.cutoff=60), echo=FALSE}
#sum(!is.na(echus_1$Q5))
echus5 <- echus_1 %>% filter(Q5 !="")
head(echus5$Q5, 43)
```

